# test-getEstimates.R

test_that("getEstimates works for simulated scenarios and returns sensible structure", {
  set.seed(123)

  # Simulated scenarios with a 'historic' cohort (rr = 3) and differences
  scenarios_list <- simulateScenarios(
    n_subjects_list     = list(c(10, 20, 30)),
    response_rates_list = list(c(0.1, 0.2, 3)),  # 3 triggers 'historic' branch
    n_trials            = 10
  )

  analyses_list <- performAnalyses(
    scenario_list     = scenarios_list,
    target_rates      = c(0.1, 0.1, 0.1),
    calc_differences  = matrix(c(3, 2, 2, 1), ncol = 2),
    n_mcmc_iterations = 100
  )

  res <- getEstimates(analyses_list)

  # Result should be a list; inner-most elements must be matrices
  expect_type(res, "list")
  expect_true(length(res) > 0)

  # Get first matrix, regardless of whether listPerMethod returns list-of-lists or list-of-matrices
  first_obj <- res[[1]]
  if (is.list(first_obj)) {
    first_mat <- first_obj[[1]]
  } else {
    first_mat <- first_obj
  }

  expect_true(is.matrix(first_mat))

  # For simulated trials, we expect Mean, SD, CI cols + Bias and MSE
  expect_true(all(c("Mean", "SD", "2.5%", "50%", "97.5%", "Bias", "MSE") %in% colnames(first_mat)))

  # Row names should contain response-rate parameters (p_*) and diff cohorts
  expect_true(any(grepl("^p_", rownames(first_mat))))
  expect_true(any(grepl("diff", rownames(first_mat))))
})

test_that("additional parameters are added and have NA bias/MSE", {
  set.seed(123)

  scenarios_list <- simulateScenarios(
    n_subjects_list     = list(c(10, 20, 30)),
    response_rates_list = list(c(0.1, 0.2, 3)),
    n_trials            = 10
  )

  analyses_list <- performAnalyses(
    scenario_list     = scenarios_list,
    target_rates      = c(0.1, 0.1, 0.1),
    calc_differences  = matrix(c(3, 2, 2, 1), ncol = 2),
    n_mcmc_iterations = 100
  )

  # Baseline without additional parameters
  res_base <- getEstimates(analyses_list)
  base_obj <- res_base[[1]]
  if (is.list(base_obj)) base_mat <- base_obj[[1]] else base_mat <- base_obj

  # With additional parameters (typical for hierarchical models)
  res_add <- getEstimates(
    analyses_list   = analyses_list,
    add_parameters  = c("mu", "tau", "w_1", "w_2", "w_3")
  )
  add_obj <- res_add[[1]]
  if (is.list(add_obj)) add_mat <- add_obj[[1]] else add_mat <- add_obj

  # Structure must be the same in terms of columns
  expect_identical(colnames(base_mat), colnames(add_mat))

  # Additional rows should appear for mu, tau, w_*
  extra_rows <- setdiff(rownames(add_mat), rownames(base_mat))
  expect_true(length(extra_rows) > 0)
  expect_true(all(grepl("mu|tau|w_", extra_rows)))

  # For additional parameters, Bias and MSE must be NA (by design)
  expect_true(all(is.na(add_mat[extra_rows, "Bias"])))
  expect_true(all(is.na(add_mat[extra_rows, "MSE"])))
})

test_that("single-trial outcome returns only posterior summaries (no bias/MSE)", {
  set.seed(123)

  outcome <- createTrial(
    n_subjects   = c(10, 20, 30),
    n_responders = c(1, 2, 3)
  )

  outcome_analysis <- performAnalyses(
    scenario_list     = outcome,
    target_rates      = c(0.1, 0.1, 0.1),
    n_mcmc_iterations = 100
  )

  res_single <- getEstimates(outcome_analysis)
  single_obj <- res_single[[1]]
  if (is.list(single_obj)) single_mat <- single_obj[[1]] else single_mat <- single_obj

  # For a single trial, code path should return only Mean, SD, and CI columns
  expect_true(is.matrix(single_mat))
  expect_identical(
    colnames(single_mat),
    c("Mean", "SD", "2.5%", "50%", "97.5%")
  )
})

test_that("alpha_level and add_parameters are validated correctly", {
  set.seed(123)

  scenarios_list <- simulateScenarios(
    n_subjects_list     = list(c(10, 20, 30)),
    response_rates_list = list(c(0.1, 0.2, 3)),
    n_trials            = 5
  )

  analyses_list <- performAnalyses(
    scenario_list     = scenarios_list,
    target_rates      = c(0.1, 0.1, 0.1),
    n_mcmc_iterations = 50
  )

  # alpha_level must correspond to stored quantiles -> should error for a "weird" value
  expect_error(
    getEstimates(analyses_list, alpha_level = 0.07),
    "alpha_level.*stored quantiles",
    ignore.case = TRUE
  )

  # Additional parameters that never occur in any method must trigger the specific error
  expect_error(
    getEstimates(analyses_list, add_parameters = c("totally_unknown_param")),
    "do not occur",
    ignore.case = TRUE
  )

  # alpha_level must be in (0,1)
  expect_error(
    getEstimates(analyses_list, alpha_level = -0.1),
    "alpha_level",
    ignore.case = TRUE
  )
  expect_error(
    getEstimates(analyses_list, alpha_level = 1),
    "alpha_level",
    ignore.case = TRUE
  )
})

test_that("point_estimator argument is respected and input type is validated", {
  set.seed(123)

  scenarios_list <- simulateScenarios(
    n_subjects_list     = list(c(10, 20, 30)),
    response_rates_list = list(c(0.1, 0.2, 3)),
    n_trials            = 5
  )

  analyses_list <- performAnalyses(
    scenario_list     = scenarios_list,
    target_rates      = c(0.1, 0.1, 0.1),
    n_mcmc_iterations = 50
  )

  # Both median and mean should work and produce same shape
  res_median <- getEstimates(analyses_list, point_estimator = "median")
  res_mean   <- getEstimates(analyses_list, point_estimator = "mean")

  med_obj <- res_median[[1]]
  mean_obj <- res_mean[[1]]
  if (is.list(med_obj)) med_mat <- med_obj[[1]] else med_mat <- med_obj
  if (is.list(mean_obj)) mean_mat <- mean_obj[[1]] else mean_mat <- mean_obj

  expect_identical(dim(med_mat), dim(mean_mat))
  expect_identical(colnames(med_mat), colnames(mean_mat))

  # Invalid point_estimator must error via checkmate::assertChoice
  expect_error(
    getEstimates(analyses_list, point_estimator = "mode"),
    "point_estimator",
    ignore.case = TRUE
  )

  # Non-analysis_list input must fail the class assertion
  expect_error(
    getEstimates(list(a = 1)),
    "analyses_list",
    ignore.case = TRUE
  )
})
