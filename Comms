# Tests for calcDiffsMCMC ------------------------------------------------------

test_that("calcDiffsMCMC: adds correctly named difference columns with correct values", {
  # Posterior samples: 2 rows, 3 p-columns + one non-p column (mu)
  posterior_samples <- cbind(
    p_1 = c(0.1, 0.2),
    p_2 = c(0.5, 0.6),
    p_3 = c(0.8, 0.9),
    mu  = c(1.0, 2.0)
  )
  
  # We want p_diff_12 = p_1 - p_2 and p_diff_31 = p_3 - p_1
  calc_differences <- rbind(
    c(1, 2),
    c(3, 1)
  )
  
  out <- calcDiffsMCMC(
    posterior_samples = posterior_samples,
    calc_differences  = calc_differences
  )
  
  # Original columns must still be present
  expect_true(all(c("p_1", "p_2", "p_3", "mu") %in% colnames(out)))
  
  # New diff columns must be appended
  expect_true(all(c("p_diff_12", "p_diff_31") %in% colnames(out)))
  
  # Check values
  expected_diff_12 <- posterior_samples[, "p_1"] - posterior_samples[, "p_2"]
  expected_diff_31 <- posterior_samples[, "p_3"] - posterior_samples[, "p_1"]
  
  expect_equal(out[, "p_diff_12"], expected_diff_12)
  expect_equal(out[, "p_diff_31"], expected_diff_31)
})


# Tests for prepareAnalysis ----------------------------------------------------

test_that("prepareAnalysis: berry branch builds correct j_data and parameters", {
  target_rates <- c(0.5, 0.7)
  priors_list  <- getPriorParameters(method_names = "berry", target_rates = target_rates)
  priors_berry <- priors_list[["berry"]]
  
  prep <- prepareAnalysis(
    method_name     = "berry",
    prior_parameters = priors_berry,
    target_rates     = target_rates
  )
  
  j_data <- prep$j_data
  
  # j_data contents
  expect_equal(j_data$mean_mu,      priors_berry$mu_mean)
  expect_equal(j_data$precision_mu, priors_berry$mu_sd^-2)
  expect_equal(j_data$precision_tau, priors_berry$tau_scale^-2)
  expect_equal(j_data$p_t,          target_rates)
  expect_equal(j_data$J,            length(target_rates))
  
  # JAGS config
  expect_equal(prep$j_parameters, c("p", "mu", "tau"))
  expect_true(is.character(prep$j_model_file))
  expect_true(file.exists(prep$j_model_file))
})

test_that("prepareAnalysis: exnex branch builds mixture priors and pMix", {
  target_rates <- c(0.4, 0.6, 0.8)
  priors_list  <- getPriorParameters(method_names = "exnex", target_rates = target_rates)
  priors_exnex <- priors_list[["exnex"]]
  
  prep <- prepareAnalysis(
    method_name     = "exnex",
    prior_parameters = priors_exnex,
    target_rates     = target_rates
  )
  
  j_data <- prep$j_data
  
  Nexch <- length(priors_exnex$mu_mean)
  Nstrata <- length(priors_exnex$mu_j)
  
  expect_equal(j_data$Nexch, Nexch)
  expect_equal(j_data$Nmix,  Nexch + 1L)
  expect_equal(j_data$Nstrata, Nstrata)
  
  expect_equal(j_data$mu_mean, priors_exnex$mu_mean)
  expect_equal(j_data$mu_prec, priors_exnex$mu_sd^-2)
  
  expect_equal(j_data$tau_HN_scale,
               rep(priors_exnex$tau_scale, Nexch))
  
  expect_equal(j_data$nex_mean, priors_exnex$mu_j)
  expect_equal(j_data$nex_prec, priors_exnex$tau_j^-2)
  
  expect_length(j_data$pMix, Nexch + 1L)
  expect_equal(sum(j_data$pMix), 1, tolerance = 1e-8)
  
  expect_equal(prep$j_parameters, c("p", "mu", "tau", "exch"))
  expect_true(file.exists(prep$j_model_file))
})

test_that("prepareAnalysis: exnex_adj branch includes p_target and uses exnex_adj model", {
  target_rates <- c(0.5, 0.6)
  priors_list  <- getPriorParameters(method_names = "exnex_adj", target_rates = target_rates)
  priors       <- priors_list[["exnex_adj"]]
  
  prep <- prepareAnalysis(
    method_name     = "exnex_adj",
    prior_parameters = priors,
    target_rates     = target_rates
  )
  
  j_data <- prep$j_data
  
  # Same structure as exnex, plus p_target
  expect_equal(j_data$p_target, target_rates)
  expect_equal(prep$j_parameters, c("p", "mu", "tau", "exch"))
  expect_true(grepl("exnex_adj", prep$j_model_file))
  expect_true(file.exists(prep$j_model_file))
})

test_that("prepareAnalysis: stratified and pooled use dummy JAGS info and pass priors through", {
  target_rates <- c(0.5, 0.5)
  priors_list  <- getPriorParameters(
    method_names = c("stratified", "pooled"),
    target_rates = target_rates
  )
  
  # stratified
  prep_strat <- prepareAnalysis(
    method_name      = "stratified",
    prior_parameters = priors_list[["stratified"]],
    target_rates     = target_rates
  )
  expect_equal(prep_strat$j_model_file, "dummy path to JAGS model")
  expect_equal(prep_strat$j_parameters, "dummy JAGS parameters")
  expect_identical(prep_strat$j_data, priors_list[["stratified"]])
  
  # pooled
  prep_pooled <- prepareAnalysis(
    method_name      = "pooled",
    prior_parameters = priors_list[["pooled"]],
    target_rates     = target_rates
  )
  expect_equal(prep_pooled$j_model_file, "dummy path to JAGS model")
  expect_equal(prep_pooled$j_parameters, "dummy JAGS parameters")
  expect_identical(prep_pooled$j_data, priors_list[["pooled"]])
})

test_that("prepareAnalysis: invalid method_name throws an error", {
  expect_error(
    prepareAnalysis(method_name = "invalid_method"),
    "method_name must be one of"
  )
})


# Tests for getUniqueRows ------------------------------------------------------

test_that("getUniqueRows: returns unique row combinations with correct columns", {
  mat <- rbind(
    c(1, 2),
    c(1, 2),
    c(2, 3),
    c(2, 3),
    c(4, 5)
  )
  
  out <- getUniqueRows(mat)
  
  # Same number of columns
  expect_equal(ncol(out), ncol(mat))
  
  # All rows in 'out' must be unique
  out_df <- as.data.frame(out)
  expect_equal(nrow(out_df), nrow(unique(out_df)))
  
  # The set of unique rows should match base::unique(mat)
  exp_df <- as.data.frame(unique(mat))
  
  out_ord <- out_df[do.call(order, out_df), , drop = FALSE]
  exp_ord <- exp_df[do.call(order, exp_df), , drop = FALSE]
  
  expect_equal(out_ord, exp_ord)
})


# Tests for getUniqueTrials ----------------------------------------------------

test_that("getUniqueTrials: combines scenarios and returns unique responder/subject/go rows", {
  # Manually define a tiny scenario_list with 2 scenarios and overlapping rows
  
  scenario_list <- list(
    scenario_1 = list(
      n_responders = rbind(
        c(1, 2),
        c(1, 2)
      ),
      n_subjects = rbind(
        c(10, 10),
        c(10, 10)
      ),
      previous_analyses = list(
        go_decisions = cbind(c(1, 0))
      )
    ),
    scenario_2 = list(
      n_responders = rbind(
        c(1, 2),
        c(2, 1)
      ),
      n_subjects = rbind(
        c(10, 10),
        c(10, 10)
      ),
      previous_analyses = list(
        go_decisions = cbind(c(1, 1))
      )
    )
  )
  class(scenario_list) <- "scenario_list"
  
  out <- getUniqueTrials(scenario_list)
  
  # Columns: responders (2), subjects (2), go_flag (1) => total 5
  expect_equal(ncol(out), 5)
  
  # No duplicate rows
  out_df <- as.data.frame(out)
  expect_equal(nrow(out_df), nrow(unique(out_df)))
  
  # All returned rows must be present in the combined raw matrix
  all_resp <- do.call(rbind, lapply(scenario_list, function(x) x$n_responders))
  all_subj <- do.call(rbind, lapply(scenario_list, function(x) x$n_subjects))
  all_go   <- do.call(rbind, lapply(scenario_list, function(x) x$previous_analyses$go_decisions))[, 1]
  
  combined <- cbind(all_resp, all_subj, go_flag = all_go)
  comb_df  <- as.data.frame(combined)
  
  # Each unique row in 'out' should occur in the original combined matrix
  merged <- merge(out_df, comb_df)
  expect_equal(nrow(merged), nrow(out_df))
})


# Tests for mapUniqueTrials ----------------------------------------------------

test_that("mapUniqueTrials: without previous trials, maps unique trial quantiles back per scenario", {
  foreach::registerDoSEQ()
  
  # One scenario with 2 trial realizations, 2 cohorts
  n_resp <- rbind(
    c(1, 2),
    c(2, 1)
  )
  n_subj <- rbind(
    c(10, 10),
    c(10, 10)
  )
  
  scenario_list <- list(
    scenario_1 = list(
      scenario_number  = 1,
      n_responders     = n_resp,
      n_subjects       = n_subj
      # no previous_analyses needed, as applicable_previous_trials = FALSE
    )
  )
  class(scenario_list) <- "scenario_list"
  
  # Unique trials (here: just all rows of [n_resp | n_subj])
  trials_unique_calc <- cbind(n_resp, n_subj)
  
  # Fake method quantiles: one matrix per unique trial
  q1 <- matrix(1:4, nrow = 2,
               dimnames = list(c("Mean", "SD"), c("p_1", "p_2")))
  q2 <- matrix(5:8, nrow = 2,
               dimnames = list(c("Mean", "SD"), c("p_1", "p_2")))
  
  method_quantiles_list <- list(
    pooled = list(q1, q2)
  )
  
  # Call mapUniqueTrials (no previous analyses used)
  out <- mapUniqueTrials(
    scenario_list              = scenario_list,
    method_quantiles_list      = method_quantiles_list,
    trials_unique_calc         = trials_unique_calc,
    applicable_previous_trials = FALSE
  )
  
  expect_type(out, "list")
  expect_identical(names(out), "scenario_1")
  
  scen1 <- out[["scenario_1"]]
  expect_true(is.list(scen1))
  expect_true("pooled" %in% names(scen1))
  
  # We expect two elements (one per trial) with quantiles equal to q1, q2
  expect_identical(scen1$pooled[[1]], q1)
  expect_identical(scen1$pooled[[2]], q2)
})

test_that("mapUniqueTrials: with previous trials, only GO trials are updated from hash tables", {
  foreach::registerDoSEQ()
  
  # 2 trials, 1 cohort for simplicity
  n_resp <- rbind(1, 2)
  n_subj <- rbind(10, 10)
  
  # previous_analyses with simple go_decisions and existing post_quantiles
  prev_post <- list(
    pooled = list(
      matrix("prev1", nrow = 1, dimnames = list("dummy", "p_1")),
      matrix("prev2", nrow = 1, dimnames = list("dummy", "p_1"))
    )
  )
  
  scenario_list <- list(
    scenario_1 = list(
      scenario_number   = 1,
      n_responders      = n_resp,
      n_subjects        = n_subj,
      previous_analyses = list(
        go_decisions = cbind(c(1, 0)),  # first trial GO, second trial NoGo
        post_quantiles = prev_post
      )
    )
  )
  class(scenario_list) <- "scenario_list"
  
  # Unique trials (both rows)
  trials_unique_calc <- cbind(n_resp, n_subj)
  
  # New quantiles we want to use for GO trials
  new_q1 <- matrix("new1", nrow = 1, dimnames = list("dummy", "p_1"))
  new_q2 <- matrix("new2", nrow = 1, dimnames = list("dummy", "p_1"))
  
  method_quantiles_list <- list(
    pooled = list(new_q1, new_q2)
  )
  
  out <- mapUniqueTrials(
    scenario_list              = scenario_list,
    method_quantiles_list      = method_quantiles_list,
    trials_unique_calc         = trials_unique_calc,
    applicable_previous_trials = TRUE
  )
  
  scen1 <- out[["scenario_1"]]
  expect_true(is.list(scen1))
  expect_true("pooled" %in% names(scen1))
  
  # For GO trial (row 1), we expect the updated quantiles (new_q1)
  expect_identical(scen1$pooled[[1]], new_q1)
  # For NoGo trial (row 2), the old quantiles should remain (prev2)
  expect_identical(scen1$pooled[[2]], prev_post$pooled[[2]])
})


# Tests for posteriors2Quantiles ----------------------------------------------

test_that("posteriors2Quantiles: computes quantiles, mean, and sd for each column", {
  set.seed(123)
  
  # One parameter (theta) with known distribution: Normal(2, 3)
  theta <- rnorm(5000, mean = 2, sd = 3)
  post  <- cbind(theta = theta)
  
  quantiles <- c(0.25, 0.5, 0.75)
  
  out <- posteriors2Quantiles(
    quantiles  = quantiles,
    posteriors = post
  )
  
  # Structure
  expect_true(is.matrix(out))
  expect_identical(colnames(out), "theta")
  expect_identical(rownames(out),
                   c("25%", "50%", "75%", "Mean", "SD"))
  
  # Check quantiles vs base::quantile
  exp_q <- stats::quantile(theta, probs = quantiles)
  expect_equal(out[c("25%", "50%", "75%"), "theta"],
               exp_q,
               tolerance = 1e-2)  # small Monte Carlo noise
  
  # Mean & SD vs sample mean & sd
  expect_equal(out["Mean", "theta"], mean(theta), tolerance = 1e-2)
  expect_equal(out["SD",   "theta"], stats::sd(theta), tolerance = 1e-2)
})
